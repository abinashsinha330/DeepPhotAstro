1. https://pathmind.com/wiki/attention-mechanism-memory-network#ffn:
  1. Feed forward networks treat features as independent (eg. feature os gender and siblings)
  2. convolutional networks focus on relative location and proximity; 
  3. RNNs and LSTMs have memory limitations and tend to read in one direction. 
  4. In contrast to these, attention and the transformer can grab context about a word from distant parts of a sentence, 
    both earlier and later than the word appears, in order to encode information to help us understand the word and 
    its role in the system called a sentence
2. https://medium.com/inside-machine-learning/what-is-a-transformer-d07dd1fbec04
3. *https://towardsdatascience.com/attention-for-time-series-classification-and-forecasting-261723e0006d*:
  In industry many data scientists still utilize simple autoregressive models instead of deep learning. 
  Usually, the common reasons for choosing these methods remain interpretability, limited data, ease of use, and training cost.
  However, deep models with attention mechanism provide interpretability in terms of attention heatmaps and are faster than
  RNNs.
4. 
